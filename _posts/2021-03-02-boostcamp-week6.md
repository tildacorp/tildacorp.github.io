---
layout: post
title: "6주차 특강 리뷰"
subtitle: "실제 모델 적용 사례/산업에서의 니즈"
date: 2021-03-02 18:15:12+0900
background: '/img/posts/bg-posts.png'
use_math: true
---

## 개요 <!-- omit in toc -->
> 6주차 특강들에 대한 리뷰를 정리해보았다.  
  
- [이활석 님 특강](#이활석-님-특강)
- [박은정 님 특강](#박은정-님-특강)
- [김상훈 님 특강](#김상훈-님-특강)
- [이준엽 님 특강](#이준엽-님-특강)
- [박성준 님 특강](#박성준-님-특강)
- [문지형 님 특강](#문지형-님-특강)
- [구종만 님 특강](#구종만-님-특강)
- [오혜연 님 특강](#오혜연-님-특강)

<br/>

## 이활석 님 특강
- 예전에도 계속 들어왔던 말이지만, 실제 AI 모델을 서빙까지 하는데 있어 AI modeling이 차지하는 비율은 그리 크지 않다. 실제로는 데이터 수집이나 서빙 환경 구축 단계에서도 많은 리소스(금전적, 시간적)를 소모하게 된다.
  
- 새로운 것에 빠르게 적응할 수 있어야 하며 이를 두려워하지 말아야한다. 회사(특히 AI 분야에서)에서 선호하는 인재상은 러닝커브가 좋은 사람이다. 

- 그리고 러닝 커브를 좋게 만들기 위해 기본기를 다지는 것이 역시나 중요할 것 같다. 베이스라인이 좋은 사람은 뭐든지 빠르게 습득할 수 있다.

- 몇 년후를 생각해보면, 단순 AI 모델링 외 다른 것도 할 줄 알아야 경쟁력이 생긴다. 
    + 지금 당장은 모델링하는 사람조차도 부족한 상황이지만, 나중에는 모델링하는 사람이 충분히 많아질 수 있고 또한 AutoML 등이 발전하고 있기 때문에 자신만의 차별화된 역량을 갖추어야 한다.
    + 물론 그렇다고 모델링 스킬을 소홀히해서는 안된다. 우선순위를 잘 파악한 상태에서 여러가지를 시도하자.

- AI 트렌드 캐치를 위해 많은 커뮤니티를 참고할 수 있는데 Tensorflow KR, 트위터, reddit, 뉴스레터(papers with code) 등이 해당된다.

- 항상 기술 변화에 민감하게 살자. 남들보다 한 박자 빠르게 뛰어들 수 있는 용기나 혜안, 그리고 무엇보다도 새로운 기술이 나타났을때 비즈니스에서 쓸모가 있는지 파악할 수 있는 역량 등도 중요하다. 이러한 역량을 키우기 위한 습관 1순위가 트렌드를 항상 파악하고 있는 것이다.

- 나름의 방향을 정하는 것이 중요하다. 사실 지금 이 분야에서 현업에 있는 사람들도 객관화된 로드맵 같은 것 없이 공부하고 있다. 이러한 것들을 스스로 설계해야한다.

<br />

---

<br />

## 박은정 님 특강
- 기본기에 탄탄해지자. 최신 모델을 아는 것도 중요하지만 그것보다도 기본적으로 베이스 지식들을 알고 있어야한다.   
    + 예를 들어 learning rate, regularization 등에 대해 대충 뭐하는건지만 알고 있지 말고 이들을 최적화시키는 여러가지 방법에 대해 고민해보고 찾아보자. 여기서부터 모든 것이 파생돼서 나오는건데 이런거를 모르는 채로 최신 기술들만 알려고 하면 안된다. 
    + 기술의 기본기를 쌓기 위해 어떤 개념이 있으면 그 개념의 근본적인 시작점에 대해 고민해보자.

- 기초 과목에 대한 지식도 중요하지만 실전 경험 역시 매우 중요하다. Kaggle이나 데이콘 등에 꾸준히 참여하려고 하자.

- 자신한테 맞는 분야를 찾기 위해 각자에게 맞는 방법이 있다. 오픈소스를 뜯어보고 contribution을 하거나 Kaggle에 참여하거나 논문을 재구현해보거나 여러가지 방법론이 있으니 직접 해보자.

- 한 분야에만 너무 몰두하는 것도 좋지만, 기회가 왔을 때 빠르게 전환하는 것도 중요한 것 같다. 
    + 물론 한 분야를 파서 더 성공할 수도 있는 것이지만, 지금까지 쌓아온 것이 아깝다고 기회가 왔을때 그걸 버리는 일은 없도록 하자. 
    + 항상 기회가 왔을 때 그쪽으로의 전환에 대해 두려워하지 말자.

- 뭘 좋아하는지 모르겠으면 뭘 싫어하는지를 먼저 생각해보자.   
    
- 직접 모델을 짜보고 배포까지 해보는 훈련을 하자.

<br />

---

<br />

## 김상훈 님 특강  
- Kaggle 등 경진대회에 꾸준히 참여하면 성과가 나오지 않더라도 실력 향상에 큰 도움이 될 수 있다.  

- GPU가 없더라도 colab pro 등을 이용하여 우승까지 하는 사람도 있다(...) 환경이 좋지 않더라도 도전해볼만한 가치는 있다.

- 공부 목적에서는 kaggle notebook만 잘 활용하더라도 꽤 좋은 공부를 할 수 있다.

- 하이퍼 파라미터는 어떤 데이터를 다루느냐에 따라 조절할 수 있는 반경이 다르다. 다만 이것도 많이 참여하다보면 감각이 생긴다.

<br />

---

<br />

## 이준엽 님 특강
- 뭐든지 완성을 하려고 하자. 쉽게 지칠 수 있기 때문이다. 너무 퀄리티를 높이려고 하지 말고 시작 단계에서 너무 욕심을 부리지 말자.

- 시작하는 단계에서는 너무 깊이 알려고 하거나 모든걸 이해하려고 하지 말자. 역시 쉽게 지칠 수 있기 때문이다. 만약 한 30% 정도를 조금 봐선 도저히 모르겠다 하면 그 부분은 나중의 나에게 미뤄도 된다.

- 풀스택을 지향하더라도 자신의 메인 분야를 절대 놓치면 안된다. 서브 분야들은 내가 원하는걸 어느정도 찾아보면서 구현할 수 있을 정도 까지만 스택을 쌓으면 된다.   
  
<br />

---

<br />

## 박성준 님 특강
박성준 마스터님 특강에서는 NLP 분야에 통용되는 상식들을 얻을 수 있었다.
- **ELMo 모델**에 대하여 이전에 살펴보지 않았는데, Bidirectional Language Model의 일종으로, **워드 임베딩 시 앞쪽의 정보만 고려하고 뒤쪽의 정보는 고려하지 못한다는 단점을 해소한 방법이다.**
    + 이전에 NLP 다룰 때 잠깐 이런 방법이 있다고만 얘기하고 넘어갔었는데, ELMo가 바로 그것 중 하나이다.
    + **양방향 RNN과는 개념상으로 아예 다르다.**
    + ELMo로 만든 벡터를 기존 워드 임베딩(Word2Vec, GloVe)과 연결시켜 **ELMo representation**을 만들어 이를 최종적인 워드 임베딩으로 활용할 수 있다.

- NLP는 BiLM 이후 pre-training에 집중하게 되었다. 즉, 언어 모델만 잘 만들면 모든 task에 사용 가능하다는 점을 발견한 것이다.

- 언어 모델 평가를 위해 **GLUE(General Language Understanding Evaluation) 벤치마크**를 통상적으로 활용한다. GLUE의 척도는 아래와 같다. (SQuAD는 GLUE에 포함되지 않지만, 대표적인 NLP 모델 평가 척도 중 하나이다)  
    + Quora Question Pairs (QQP, 문장 유사도 평가)
    + Question NLI (QNLI, 자연어 추론)
    + The Stanford Sentiment Treebank (SST, 감성 분석)
    + The Corpus of Linguistic Acceptability (CoLA, 언어 수용성)
    + Semantic Textual Similarity Benchmark (STS-B, 문장 유사도 평가)
    + Microsoft Research Paraphrase Corpus (MRPC, 문장 유사도 평가)
    + Recognizing Textual Entailment (RTE,자연어 추론)
    + SQAUD 1.1/2.0 (질의응답)
    + MultiNLI Matched (자연어 추론)
    + MultiNLI Mismatched (자연어 추론)
    + Winograd NLI (자연어 추론)

- 그 외 많은 언어들에도 그만의 평가 척도(프랑스어, FLUE / 중국어, CLUE)가 있으며, KLUE(Korean)는 현재 개발중이라고..

<br />

---

<br />

## 문지형 님 특강
- 좋은 AI 모델은 좋은 데이터로부터 나온다. 그러나 데이터 수집 및 활용에 있어 저작권에 대한 충분한 고려가 필수적이다. 

- 하지만 실제 법으로는 AI 데이터 수집 관련 법이 확실하게 규정된게 아직 없어 특히 NLP 분야에서 여러모로 애매한 것 같다. 이미지는 그래도 관련 법이 좀 있지만 NLP는.. :cry: 
  
- 합법적으로 데이터를 이용하는 방법?
    + 저작자와 협의 (독점적/비독점적, 일부/전부 양도 계약서), 일정 기간을 정하여 타인의 저작재산권을 양수받아서 이용할 수 있다. 
    + 라이센스를 활용 .. 라이센스는 저작자에게 이용 허가 요청을 하지 않아도 저작자가 제안한 특정 조건을 만족하면 이용이 가능하도록 만든 저작물에 대한 이용허락 규약
    + 가장 유명한 것으로는 CCL(Creative Commons License), 공공누리(국내 문화체육관광부 제공) 등이 있다.  
  
- CCL(Creative Commons License)  

    ![CCL](/img/posts/26-1.png){: width="70%" height="70%"}{: .center}   
    + 위와 같이 대표적으로 6종류가 있다. 맨 앞 CC는 CCL, BY는 '저작자표시'로, BY는 기본 옵션이다.
        - BY: Attribution
            + 저작자 표시. 적절한 출처와 해당 라이센스 링크를 표시하고 변경이 있는 경우 공지
        - ND: NoDerivatives
            + 변경 금지. 이 저작물을 리믹스, 변형하거나 2차적 저작물을 작성하였을 경우 그 결과물을 공유할 수 없음.
        - NC: NonCommercial
            + 비영리. 이 저작물은 영리 목적 사용이 불가(비영리 목적은 교육과 연구 등에 해당)
        - SA: ShareAlike
            + 동일조건 변경허락. 이 저작물을 리믹스, 변형하거나 2차적 저작물을 작성하고 그 결과물을 공유할 경우 원 저작물과 동일한 조건의 CCL 적용
    + 예를 들어 아래와 같이 나무위키 하단에서는 CC-BY-NC-SA 라이센스를 찾아볼 수 있다.
        ![CCL_namuwiki](/img/posts/26-2.png){: width="90%" height="90%"}{: .center}   
        - 학교 소속이라면 비영리 목적(NC)으로 크롤링하여 데이터를 활용할 수 있다. 다만 동일 라이센스(CC-BY-NC-SA)를 부착하고 원 데이터의 출처를 명시(BY) 후 활용할 수 있다.  
   
- 마스터 클래스를 들어보니 개인정보가 유출될 수 있는 데이터를 활용하여 학습을 한다던가 하면 문제가 될 수 있는 것 같다. 그런데 관련 법이 확실하게 나오지 않아 되게 여러모로 애매한 것 같다.
  
- 공정 이용 (Fair-use)
    + 저작권법 상 '공정 이용'에 해당하면 저작권자의 허락을 받지 않고도 저작물을 이용할 수 있다.
    + 교육, 학교 교육 목적 등도 이에 해당한다.
    + 다만 학생 개인이 포트폴리오를 위해 데이터를 수집할 때 당연히 이를 교육 목적이라고는 할 수 없다. :cry: 보통 교육 기관에 종사하는 사람이 교육을 위해 저작물을 활용하는 경우를 뜻한다.  
  
- 언론사 기사 활용  
    + 기사의 원문은 저작권 법에 걸리지만 **기사의 제목만 사용한다면 저작권이 붙지 않아 마음대로 사용할 수 있다.** 기사 원문 외에도 헌법이나 판례 등 국가/지방자치단체가 내놓은 행정문서(?)들은 저작권법에 의해 보호받지 않는다. (마음대로 사용해도 된다)
    + 다만 뉴스 기사 자체(즉 원문)의 저작권은 언론사에 있으며 메이저 언론사(조중동)를 제외하고는 대부분 한국언론진흥재단에서 위탁 관리한다.
    + 아주 드물게 CCL이 적용된 언론사(i.e. 위키트리)도 있다.  
  
<br />

---

<br />

## 구종만 님 특강  
- 현재 딥러닝이 실제 퀀트 트레이딩에 깊게 사용되지는 않는다. 실제로는 아직도 선형회귀가 많이 사용된다.

- 상품 가격에 이미 모든 정보가 선반영되어있을 수 있다는 점, 세상에는 수많은 예측 불가 요소가 많다는 점, 내 자신의 거래도 시장에 영향을 미친다는 점 등 고려할 것이 너무나 많기 때문에 딥러닝만으로 모델에 정보를 집어넣어 가격을 예측하는 트레이딩 모델을 한번에 설계하는 것은 너무 어렵다.
    + 반대로 NLP, 사진 분류 등은 그 규칙이 오랜 시간동안 변화하지 않는다. 하지만 시장의 요소들은 끝없이 변화한다.
    + 코로나, 정부 규제, 스캔들, 타 참가자들의 접근 방법 고도화(이로 인한 시장 변화), 새로운 시장 및 상품군 출현 등

- 특히 모델을 어느정도 개발했다고 해도 오버피팅이 발생할 확률이 너무나 높으며, 미래 데이터에 대하여 오버피팅이 발생할지 미리 알 수 없다.

- 가장 쉬운 방법은 내가 무엇을 모델링하는지, 왜 그것이 의미있는지 알고 있는 것이다. (근데 이게 제일 어려운 것인데.. :cry: )

- 뭐든지 문제에 대해 깊이 이해하고 있는 것이 중요하다.

- 이런 기회가 있을 때 수업 내용을 모두 소화하는 것도 중요하지만, 자신만의 목표에 대한 고민도 계속 해야한다. 

- 의외로 경제학에 대한 지식 없이 성공하는 알고리즘도 존재한다. (말그대로 그냥 숫자 자체로 보는 것) 하지만 마켓에 대한 최소한의 이해는 필요하긴하다.

<br />

---

<br />

## 오혜연 님 특강  
- 데이터를 수집하는데에 있어 이미 bias가 많이 존재할 수 있다. 동시에 데이터가 아닌 모델 자체에서도 예상치 못한 곳에서 bias를 가질 수 있다. - i.e., 이루다 등

- 모델 개발(efficiency)도 중요하지만, 모델 학습 시 환경적 요소(energy cost, CO2 배출량 등)도 반드시 고려해야한다. GPT-3나 BERT 같은 거대 모델들은 학습을 한 번만 진행해도 배출되는 온실가스량이 엄청날 것.

- AI Ethics의 쟁점은 생성 모델의 생성 결과에 대한 것뿐만 아니라, 모델이 환경적/사회적으로 미치는 영향 모든 것을 포괄한다.
    