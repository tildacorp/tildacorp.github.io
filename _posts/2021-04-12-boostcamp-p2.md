---
layout: post
title: "P Stage - 2. Tabular Data Classification"
subtitle: "정형 데이터 분류 문제"
date: 2021-04-12 20:00:12+0900
background: '/img/posts/bg-posts.png'
use_math: true
---

## 개요 <!-- omit in toc -->
> 정형 데이터 분류 문제
  
일자별로 기록하였음  
- [Day 1 (4/12)](#day-1-412)
    - [시도](#시도)
    - [놓쳤던 점](#놓쳤던-점)
    - [부족한 점 & 해볼 것](#부족한-점--해볼-것)
    - [느낀 점](#느낀-점)
- [Day 2 (4/13)](#day-2-413)
    - [시도](#시도-1)
    - [놓쳤던 점](#놓쳤던-점-1)
    - [부족한 점 & 해볼 것](#부족한-점--해볼-것-1)
- [Day 3 (4/14)](#day-3-414)
    - [시도](#시도-2)
    - [부족한 점 & 해볼 것](#부족한-점--해볼-것-2)
- [Day 4 (4/15)](#day-4-415)
    - [시도](#시도-3)
- [Day 5 (4/16)](#day-5-416)
    - [시도](#시도-4)
    - [부족한 점 & 해볼 것](#부족한-점--해볼-것-3)
- [Day 6 (4/17~19)](#day-6-41719)
    - [시도](#시도-5)
    - [놓쳤던 점](#놓쳤던-점-2)
    - [부족한 점 & 해볼 것](#부족한-점--해볼-것-4)
- [Day 7 (4/20)](#day-7-420)
    - [시도](#시도-6)
    - [부족한 점 & 해볼 것](#부족한-점--해볼-것-5)
- [Day 8 (4/21)](#day-8-421)
    - [시도](#시도-7)
    - [놓쳤던 점](#놓쳤던-점-3)
    - [부족한 점 & 해볼 것](#부족한-점--해볼-것-6)
- [Day 9 (4/22)](#day-9-422)
    - [시도](#시도-8)
    - [놓쳤던 점](#놓쳤던-점-4)
    - [부족한 점 & 해볼 것](#부족한-점--해볼-것-7)
- [총평](#총평)
- [Reference](#reference)

<br />
  
## Day 1 (4/12)
#### 시도
- country는 얼마나 영향을 줄까? 영국과 영국 아닌 것으로 구분하는것은 어떨지?
- order_data는 월별로 합해서 가져가는 것이 좋긴 한데.. 시계열 분석으로 문제를 바라볼 경우 월별로 합하는 것은 좋지 않은 선택인 것 같다.
- 데이터를 보면 300이상 구매하는 애들은 계속해서 300이상 구매하고, 반대로 300 이하 구매자들은 300 이상의 구매가 매우 드물다.
- 내가 보기엔, (1) 그 달에 해당 물건을 구매할 것인가? (2) 구매를 300 이상 하는가? 순으로 생각을 해보아야할 것 같다. 이 경우, 먼저 이진 분류를 수행하고 회귀를 푸는 문제가 될 수도 있다. 혹은 이진 분류를 두 번하는 문제가 될 수 있다.    
  
#### 놓쳤던 점
pandas 데이터는 확실히 주피터노트북에서 찍어보면서 하는게 효율이 훨씬 좋은 것 같다. -_-;; 너무 모듈화에만 집중하지 말자.
물론 나중에는 코드를 옮겨야겠지만 차라리 주피터노트북에서 먼저 해보는게 나을 것 같다.   
  
#### 부족한 점 & 해볼 것
나만의 베이스라인 코드를 먼저 가공 및 작성하기 !!  
  
내일은 EDA를 더 진행한 이후, Kaggle에서 실제로 어떤 방식을 범용적으로 활용하는지 알아보도록 하자.  
  
- 이번 주 내에 EDA는 확실하게 어느정도 윤곽을 잡고 Kaggle에서 시계열/정형 데이터 분류를 어떤 방식으로 하고 있는지 전체적으로 윤곽을 잡는 것이 목표이다.
- 당장의 성능을 쫓지 말고 항상 차근차근 진행하도록 하자.  
  
#### 느낀 점
정형 데이터가 더 쉬울 줄 알았는데, `pandas`에 익숙치 않다보니 훨씬 어려운 느낌이 든다. ㅠㅠ  
그래도 항상 차근차근 해야된다는 점을 명심하고 앞서나가지 말자.  
  
생각보다 모델 학습이 오래 걸리지 않는다. 하이퍼 파라미터 튜닝에 많은 시간을 쏟을 수 있을지도..?
아니면 현재 피처가 얼마 되지 않아서 그런걸지도 모르겠다..
  
<br />

## Day 2 (4/13)
#### 시도
- 이건 시계열데이터라.. LightGBM, XGBoost를 쓰는게 과연 적절할까?  
(1) 24개월 지출내역을 월별로 column으로 만들어 부스팅모델에 넣기. 평균값과 std, skew등도 같이 넣어주면 좋을듯. - 성공  
(2) 시계열 데이터를 캐글에서는 어떻게 처리하고 있는가? 지금처럼 수요예측 이런거면 더 좋을듯. - 이건.. 잘 모르겠음.. 이렇게 sparse하고 예측하려는게 개인의 것인게 있나?  
(3) 월별로 정리한 데이터를 다른 모델에서 사용할 수는 없을까? LSTM 등.  
- LSTM은 sparse에 적합하지 않을 것 같다.  
- time series에서 RNN을 쓰는 이유가 뭘까? 이건 사실 window 사이즈를 적용해주는 효과가 있기 때문.  
- 따라서 지금처럼 time series 자체가 길지 않을 경우 이런 부분을 하나의 feature로 생성하여 처리해줘도 됨.  
   
#### 놓쳤던 점  
- threshold를 0.1로 줘보기. 근데 이거 결과가 왜 이럴까? => 0.2 이하만 0으로 만드는게 제일 좋았던걸로   
   
#### 부족한 점 & 해볼 것
- ARIMA 돌려보기  
- CNN 기반 모델 이해해보기  
- 클러스터링 이후 SMOTE 등을 이용하여 minor data를 augmentation
    
<br /> 
  
## Day 3 (4/14)
#### 시도
- ARIMA/CNN은 아무리 생각해봐도 좀 아닌거같아서 안함.  
- 대신 TabNet을 돌려보았음. 근데 성능이 생각보다 괜찮게 나오긴함.  
- 이정도면 LSTM 돌려볼 정도는 되지 않을까..?  
  
#### 부족한 점 & 해볼 것
- LSTM 설계하고 돌려보기  
- TabNet 이해해보기 / Informer 모델도 함 봐보자 -> feature가 지금처럼 적을때도 이게 효율적일까?  
- 데이터 어케쓸지 좀 더 고민 .. ㅠㅠ   
  
<br /> 

## Day 4 (4/15)
#### 시도
- (1) 살 사람 분류 (2) 300 이상 분류 로 task 나누는거 좋을 것 같음. => 해봤는데 괜찮게 잘 됨.  
    
<br /> 

## Day 5 (4/16)
#### 시도
- 다시 피처를 어떻게 할지 분석하기 시작함.  
    + 피처 sum, skew로 줄이고 구매내역 0이상만 보기로 함.   
    + 아직도 피처를 어떻게할지는 고민임.  
- total만 보니까 별로인듯. quantity, total 둘다 보도록 하자.  
- 0/1로 라벨링해서 보는것도 별로임. 가격 있는 그대로 보자.  
- 근데 0/1 라벨링하면 성능 같거나 넘는게 지금 이론대로면 정상아닌가..? 잘 모르겠다 ..  
(1), (2) 튜닝 후 GridSearchCV 환경 구성하여 GridSearchCV 돌려서 성능 좀 올림.    
  
#### 부족한 점 & 해볼 것
- AutoML 돌려보기   
  
<br /> 

## Day 6 (4/17~19)
#### 시도
- AutoML 적용. pycaret을 활용합시다  
- v11. 없는 데이터를 train에서 가져오자. => 이건 AutoML 아니어도 적용할 수 있을듯?  
- 평균 300 이상인 사람 몇명인지 파악해보자.  
- v11.1-v8.2에서 total 보니까 너무 많은데 300이상인 사람이?    
- tabnet, 각종 앙상블 시도해보았으나 진전은 없다.  
      
#### 놓쳤던 점  
- 환불 threshold 20. 과연 의미있는 숫자일까? 내일은 없애볼 필요가 있음.  
- 번외로 0.1 이하 없애는거 효과가 있었다 없었다 하는데.. 기준을 낮추는 것도 좋을듯  
- validation은 대체 작동을 잘 하고 있는걸까?  
  
#### 부족한 점 & 해볼 것
- 내일은 threshold 낮추고 feature 글 보고 feature를 추가하자.  
- 어느정도 성능 나오면 optuna 활용. => validation 안되면 무쓸모
   
<br /> 

## Day 7 (4/20)
#### 시도
- permutation importance 적용해봤고 cumsum 써봤는데 결국 status 구분할 때 각 요소가 큰 의미가 없다는걸 알아버림.
- 그냥 애초에 피처를 최소한으로 줄이고 주기성 반영해야할듯.
- 그나마 status에 영향을 주는게 주기성임.
- 그리고 train/test set 구성할때 순서도 잘 고려해야함. 이거 반영이 잘 안됨 아직.  
    
#### 부족한 점 & 해볼 것
- importance 그나마 높은거로만 feature 구성  
- 주기성 반영할 수 있는 피처 추가적으로 넣기. PCA 등  
- 순서 잘 맞춰서 넣기.    
- 피처를 범주형으로 변경.  
  
<br /> 

## Day 8 (4/21)
#### 시도
- 그냥 진짜 시즌성 고려하는 피처만 만들어도 되겠는데?    
- 12년 데이터, 주변 데이터에 대한 PCA 추가? 흠    
- threshold는 300이 최선인가?, status thres는? 둘다 명확한 근거가 무엇인지?
- label을 직접 확인해보면 구매내역이 존재하는 달은 대충 5000/900, 4900/1000 정도의 분포를 보임. 이거에 맞춰서 구매내역 조사 필요. 구매 예측을 이 추세에 맞춰서 할 수 있는 방법으로 무엇이 있을까?  
- 결국 구매가 존재하냐 안하냐의 문제... 구매의 절대량이 많기 때문에 또 구매를 할거다? 이건 어폐가 있음.
- 분명 1/0으로 붙여서 그냥 하는게 더 좋을거같음. 그럼 놓친게 뭐길래 v9-2에서 실패한걸까?
- 1/0의 cumsum을 보는것도 좋을듯. 이것의 skew까지. (이부분은 sum 안봐도 됨)
- 1/0의 sum 보는거? 의미 있음. 1/0의 skew 보는거? 역시 의미 있음.  
- 이 경우 quantity는 제외한다. 어차피 total이랑 똑같음. 결국 total을 봐야하고.

#### 놓쳤던 점  
- status로 binary로 라벨 변경 결과 => 0/1로 해도 잘나옴.. 문제많다. (0.854)      
  
#### 부족한 점 & 해볼 것
- 주기성을 반영할 수 있는 cumsum을 추가할까?
- 근데 이거 99%가 말이 되나..? valid에서 .. 
- 또한 실제 라벨 자체는 대충 5100~5200/- 정도의 분포임. 이거에 맞춰서 csv 라벨 확률값 조정해도 괜찮을 거 같음.
  
<br /> 

## Day 9 (4/22)
#### 시도
- time series data에서만 뽑아낼 수 있는 피처를 활용  
(1) 최근 10개월/7개월/4개월 구매 데이터에 대한 sum/skew – 최근에 구매를 많이 했다면 계속해서 구매를 지속할 가능성이 크다.   
(2) 최근 1년간 2개월/3개월 간격 구매 데이터에 대한 sum/skew, 1년 간격 구매 데이터에 대한 sum/skew – 데이터를 보면 일정 간격으로 구매를 하는 사람들이 존재한다.  
(3) 이전 해 비슷한 시기(ex. 2010년 11월~2011년 1월) 데이터들의 sum/skew – 딱 12월 데이터만이 아니라, 비슷한 시기의 데이터들도 도움이 될 수 있다.  
    
#### 놓쳤던 점  
- 이걸 처음에는 sum과 cumsum이 아니라 count와 이에 대한 cumsum에다가 적용했었으나 그때는 어차피 지금 보고있는 것은 $300 이상 이하가 중요한게 아니고 구매 여부가 중요한 것이니 그냥 구매를 했으면 1, 안했으면 0으로 나타내고 이에 대한 피처 엔지니어링을 하는 것이 모델 학습에 더 도움이 되지 않을까라는 생각을 했다.
- 하지만 이렇게 하니 오히려 모든 사람의 데이터가 거의 비슷비슷하게 변하게 되었고 model이 일부 데이터에 overfitting되는 현상이 일어나 성능이 급격히 감소하였다.
- 해당 부분을 오늘은 encoding 안하고 값 그대로 반영하였다.

#### 부족한 점 & 해볼 것
- 위와 같은 피처는 추가하려면 계속해서 추가할 수 있을 것 같은데 제출기회 제한으로 피처를 더 추가하지 못했다.
- 더 다양한 관점에서의 계절성과 주기성을 반영할 수 있었다면 성능이 조금은 더 향상되지 않았을까?
   
<br/> 
  
## 총평
- 직관이 잘 통해서 지난 번 대회보다 순위는 떨어졌지만 좀 더 만족스러웠던 것 같다.
- 지난 번에 비해 버전 관리는 잘 되었으나 모델에 대한 이해도는 아직도 해결하지 못한 숙제이다. 다음 스테이지때는 더 잘 보완해야할 필요가 있다.
- 방법에 열중하지 말고 좀 더 근본에 열중하자. ㅋㅋㅋㅋㅋ

<br />

## Reference  
-  
  